---
title: "llama3.java"
sidebar_position: 2
hide_title: true

# REQUIRED TAGS â€” fill in all of these:

level: intermediate      # beginner / intermediate / advanced / expert
type: overview         # tutorial / overview / code / benchmark / opinion / api-doc
status: draft          # draft / review-needed / published / missing
visibility: public     # public

topics:
  - llama3
  - local-inference
  - java
  - llm

# ðŸ§© OPTIONAL TAGS:

# article-priority: high   # high / medium â€” omit if not important

# collaboration: open      # set if author welcomes collaborators
# collaboration-topic: "need help implementing Spring Boot starter examples"  
#                        # explain what help is welcome (appears on the dashboard & collab page)

# review-reason: "seems not to be on the right topic"
#                        # required when status: review-needed â€” will show on the article and in the dashboard

author: "Lize Raes (@lizeraes)"

eta: 2025-07-01           # Set only if status is draft

# Feature-related tags (only if this doc describes a feature or gap in Java+AI):
# feature-status: preview        # missing / experimental / preview / stable / specified
# feature-priority: high         # suggested / medium / high
# feature-responsible: openjdk   # community / openjdk / oracle-architects / jsr / vendor:redhat / project-lead:<name>
---

# llama3.java

llama3.java is a Java implementation for running Meta's Llama 3 models locally. It provides a lightweight and efficient way to integrate Llama 3 inference capabilities into Java applications without external dependencies.

The library offers optimized performance for Java environments and supports various Llama 3 model sizes. It's designed for developers who want to leverage the latest Llama 3 capabilities while maintaining the benefits of Java's ecosystem and tooling.

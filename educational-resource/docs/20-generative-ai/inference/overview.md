---
title: "Inference Overview"
sidebar_position: 1
hide_title: true

# REQUIRED TAGS â€” fill in all of these:

level: intermediate      # beginner / intermediate / advanced / expert
type: overview         # tutorial / overview / code / benchmark / opinion / api-doc
status: published      # draft / review-needed / published / missing
visibility: public     # public

topics:
  - inference
  - local-models
  - llm
  - java

# ðŸ§© OPTIONAL TAGS:

# article-priority: high   # high / medium â€” omit if not important

# collaboration: open      # set if author welcomes collaborators
# collaboration-topic: "need help implementing Spring Boot starter examples"  
#                        # explain what help is welcome (appears on the dashboard & collab page)

# review-reason: "seems not to be on the right topic"
#                        # required when status: review-needed â€” will show on the article and in the dashboard

author: "Lize Raes (@lizeraes)"

# eta: 2025-07-01           # Set only if status is draft

# Feature-related tags (only if this doc describes a feature or gap in Java+AI):
# feature-status: preview        # missing / experimental / preview / stable / specified
# feature-priority: high         # suggested / medium / high
# feature-responsible: openjdk   # community / openjdk / oracle-architects / jsr / vendor:redhat / project-lead:<name>
---

# Inference Overview

Local inference enables Java applications to run AI models directly on your infrastructure without relying on external APIs. This section covers the various Java libraries and tools available for local model inference.

These solutions provide benefits like reduced latency, improved privacy, and cost savings for high-volume applications. They range from lightweight implementations for simple use cases to full-featured frameworks supporting complex model architectures.
